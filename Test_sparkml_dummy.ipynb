{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import numpy as np\n",
    "import ast, csv, warnings, os, pickle, sys\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc =SparkContext()\n",
    "sqlcontext = SQLContext(sc) \n",
    "#spark 2.1.0\n",
    "#sqlcontext.setConf(\"spark.sql.shuffle.partitions\", \"10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size is 4898431\n",
      "Test data size is 311029\n",
      "peak memory: 52.30 MiB, increment: 0.23 MiB\n"
     ]
    }
   ],
   "source": [
    "#Dummy Data\n",
    "import urllib\n",
    "f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\", \"kddcup.data.gz\")\n",
    "\n",
    "data_file = \"/idn/home/lwang27/workspace/rNd/kddcup.data.gz\"\n",
    "raw_data = sc.textFile(data_file) #rdd\n",
    "\n",
    "print \"Train data size is {}\".format(raw_data.count()) #4,898,431\n",
    "\n",
    "ft = urllib.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz\", \"corrected.gz\")\n",
    "\n",
    "\n",
    "test_data_file = \"/idn/home/lwang27/workspace/rNd/corrected.gz\"\n",
    "test_raw_data = sc.textFile(test_data_file)\n",
    "\n",
    "%memit print \"Test data size is {}\".format(test_raw_data.count())#311,029"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Introduction:\n",
    "detecting network attacks\n",
    "features:\n",
    "duration\tprotocol_type\tservice\tflag\tsrc_bytes\tdst_bytes\tland\twrong_fragment\turgent\thot\tnum_failed_logins\tlogged_in\tnum_compromised\troot_shell\tsu_attempted\tnum_root\tnum_file_creations\tnum_shells\tnum_access_files\tnum_outbound_cmds\tis_host_login\tis_guest_login\tcount\tsrv_count\tserror_rate\tsrv_serror_rate\trerror_rate\tsrv_rerror_rate\tsame_srv_rate\tdiff_srv_rate\tsrv_diff_host_rate\tdst_host_count\tdst_host_srv_count\tdst_host_same_srv_rate\tdst_host_diff_srv_rate\tdst_host_same_src_port_rate\tdst_host_srv_diff_host_rate\tdst_host_serror_rate\tdst_host_srv_serror_rate\tdst_host_rerror_rate\tdst_host_srv_rerror_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0,tcp,http,SF,215,45076,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0.00,0.00,0.00,0.00,1.00,0.00,0.00,0,0,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.take(1) #42 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from numpy import array\n",
    "from numpy import array\n",
    "\n",
    "def parse_interaction(line):\n",
    "    line_split = line.split(\",\")\n",
    "    # leave_out = [1,2,3,41]\n",
    "    clean_line_split = line_split[0:1]+line_split[4:41] #continuous features\n",
    "    attack = 1.0\n",
    "    if line_split[41]=='normal.':\n",
    "        attack = 0.0\n",
    "    return (attack, Vectors.dense(array([float(x) for x in clean_line_split])))\n",
    "\n",
    "training_data = raw_data.map(parse_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.ml API support for dataframes and ML pipelines.\n",
    "#use of dataframe metadata to distinguish continous and categorical features.\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "df = training_data.toDF() #select the first 10000 rows\n",
    "#df = training_data.toDF()\n",
    "df = df.withColumnRenamed(\"_1\", \"label\")\n",
    "df = df.withColumnRenamed(\"_2\", \"features\")\n",
    "#stringIndexer = StringIndexer(inputCol = \"label\", outputCol = \"indexed\") \n",
    "##stringIndexer: encodes a string column of labels to a column of label indices.\n",
    "#si_model = stringIndexer.fit(df)\n",
    "#td = si_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Build Model using RandomForest\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "t0 = time()\n",
    "rf = RandomForestRegressor(numTrees = 3,maxDepth=3, seed = 42)\n",
    "model_rf = rf.fit(df)\n",
    "tt = time() - t0\n",
    "print model_rf.featureImportances\n",
    "print \"RF training in {} seconds\".format(np.round(tt,3)) \n",
    "## of features 38. \n",
    "#td_f = td.select('features').collect() \n",
    "#collect, return all the elements as an array\n",
    "#print len(td_f[0].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT training in 1399.916 seconds\n"
     ]
    }
   ],
   "source": [
    "#Build model using GBT\n",
    "\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "t0 = time()\n",
    "gbt = GBTRegressor(maxDepth=3, seed = 42)\n",
    "model_gbt = gbt.fit(df1)\n",
    "tt = time() - t0\n",
    "#print model_gbt.featureImportances\n",
    "print \"GBT training in {} seconds\".format(np.round(tt,3)) #640 seconds whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search & cross validation\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "t0 = time()\n",
    "gbt = GBTRegressor()\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "            .addGrid(gbt.maxDepth, [1,2,3])\\\n",
    "            .addGrid(gbt.maxIter, [5, 20])\\\n",
    "            .build()\n",
    "evaluator = RegressionEvaluator(metricName = \"r2\")\n",
    "\n",
    "crossval = CrossValidator(estimator = gbt, estimatorParamMaps = paramGrid, evaluator = evaluator, numFolds = 3)\n",
    "model_cv = crossval.fit(df)\n",
    "tt = time() - t0\n",
    "print \"cross validation in {} seconds\".format(np.round(tt,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get model metrics\n",
    "model_cv.avgMetrics\n",
    "#Average cross-validation metrics for each paramMap \n",
    "#in CrossValidator.estimatorParamMaps, in the corresponding order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare test data\n",
    "test_data = test_raw_data.map(parse_interaction)\n",
    "\n",
    "df_test = test_data.toDF()\n",
    "df_test = df_test.withColumnRenamed(\"_1\", \"label\")\n",
    "df_test = df_test.withColumnRenamed(\"_2\", \"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction based on best model\n",
    "predictions = model_cv.transform(df_test.select(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
